{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "#%pip install -q pypdf langchain langchain-community langchain_mistralai langchain-huggingface faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "if \"MISTRAL_API_KEY\" not in os.environ:\n",
    "     os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Enter your Mistral API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    pdf_reader = PdfReader(pdf_path)\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process PDF and create vector store\n",
    "def process_pdf(pdf_path):\n",
    "    # Extract text from PDF\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    texts = text_splitter.split_text(raw_text)\n",
    "    \n",
    "    # Create embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Create vector store\n",
    "    vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-large-latest\",  #\n",
    "    temperature=0.7,  # increased more creativity while maintaining focus\n",
    "    max_retries=3,  # Increased for better reliability\n",
    "    max_tokens=3000,  # Increased max output length for more comprehensive responses\n",
    "    presence_penalty=0.1,  # Slight penalty to reduce repetition\n",
    "    frequency_penalty=0.1,  # Slight penalty to encourage more diverse vocabulary\n",
    "    safe_mode=False,  # Disable safe mode if you need more flexible outputs\n",
    "    random_seed=42,  # Set a random seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 67 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 74 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 78 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 90 0 (offset 0)\n",
      "Ignoring wrong pointing object 92 0 (offset 0)\n",
      "Ignoring wrong pointing object 102 0 (offset 0)\n",
      "Ignoring wrong pointing object 104 0 (offset 0)\n",
      "Ignoring wrong pointing object 106 0 (offset 0)\n",
      "Ignoring wrong pointing object 108 0 (offset 0)\n",
      "Ignoring wrong pointing object 152 0 (offset 0)\n",
      "/var/folders/cv/7jgfzyr15ls39g7v5s_kxytm0000gn/T/ipykernel_13110/3024637175.py:16: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Process PDF and create vector store\n",
    "pdf_path = \"./docs/mtc.pdf\"\n",
    "vectorstore = process_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_chunks(query, vectorstore, k=5):\n",
    "    return vectorstore.similarity_search(query, k=k)\n",
    "\n",
    "def format_context(relevant_chunks):\n",
    "    return \"\\n\\n\".join([chunk.page_content for chunk in relevant_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cv/7jgfzyr15ls39g7v5s_kxytm0000gn/T/ipykernel_13110/1493580813.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=1000,\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chain\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    relevant_chunks = get_relevant_chunks(question, vectorstore)\n",
    "    context = format_context(relevant_chunks)\n",
    "    \n",
    "    # Get chat history from memory\n",
    "    chat_history = memory.load_memory_variables({})[\"history\"]\n",
    "    \n",
    "    # Invoke the chain\n",
    "    response = chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"question\": question\n",
    "    })\n",
    "    \n",
    "    # Save the interaction to memory\n",
    "    memory.save_context({\"input\": question}, {\"output\": response.content})\n",
    "    \n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the method use in this study?\n",
      "Answer: Based on the provided context and chat history, the study employs several methods for the multiclass mental illness classification task. Hereâ€™s a detailed overview:\n",
      "\n",
      "1. **Machine Learning Models**:\n",
      "   - **Logistic Regression**: Chosen for its simplicity and ease of interpretation.\n",
      "   - **Multinomial Naive Bayes**: Effective for text classification tasks and works well with high-dimensional datasets.\n",
      "   - **Linear Support Vector Machine (LSVM)**: Known for handling large feature spaces and strong performance in text classification tasks. Among these models, the TF-IDF with LSVM model performed best overall, achieving the highest accuracy (0.772) and F1-score (0.772).\n",
      "\n",
      "2. **Deep Learning Models**:\n",
      "   - **Bidirectional Long Short-Term Memory (BiLSTM)**: Designed with two bidirectional recurrent layers with 256 and 128 units, respectively, followed by dropout layers for regularization. It includes a dense layer with 64 units and rectified linear unit activation, preceding the final output layer with softmax activation for multiclass classification. Optimized using the Adam optimizer with a configurable learning rate.\n",
      "   - **Bidirectional Gated Recurrent Unit (BiGRU)**: Shares the same architecture as the BiLSTM model, but uses GRU layers instead. It also includes dropout layers with a rate of 0.25 for regularization. Optimized using the Adam optimizer with a configurable learning rate. Both models utilize pretrained GloVe embeddings with a 300-dimensional embedding layer and a maximum sequence length of 1024 tokens.\n",
      "\n",
      "3. **Model Evaluation Metrics**:\n",
      "   - The study uses precision, recall, and the F1-score to evaluate the models' performance, ensuring a reliable model evaluation. These metrics rely on true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
      "\n",
      "4. **Dataset Collection Method**:\n",
      "   - The dataset was collected from Reddit using the PullPush API, focusing on subreddit submissions related to mental health, including r/ADHD, r/anxiety, r/bipolar, r/BPD, r/depression, r/OCD, and r/PTSD. Posts with over ten upvotes were filtered to ensure relevance.\n",
      "\n",
      "5. **Model Performance**:\n",
      "   - The study highlights the performance of different models. For example, the BioBERT model achieved the highest accuracy (0.835), precision (0.837), recall (0.835), and F1-score (0.835) at a learning rate of 1e-5, surpassing other models like MentalBERT, BioBERT, MedBERT, and ClinicalBERT.\n",
      "\n",
      "If you have further questions or need more details about any specific method, model, or their performance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the method use in this study?\"\n",
    "answer = ask_question(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the machine learning model they use in this study?\n",
      "Answer: Based on the provided context, the study employs three well-known machine learning models for the multiclass mental illness classification task:\n",
      "\n",
      "1. **Logistic Regression**: This model is chosen for its simplicity and ease of interpretation.\n",
      "2. **Multinomial Naive Bayes**: This model is effective for text classification tasks and works well with high-dimensional datasets.\n",
      "3. **Linear Support Vector Machine (LSVM)**: This model is known for its ability to handle large feature spaces and is a strong performer in text classification tasks.\n",
      "\n",
      "These models are used as a traditional baseline approach for the classification task. Among them, the TF-IDF with LSVM model performed best overall, achieving the highest accuracy (0.772) and F1-score (0.772).\n",
      "\n",
      "If you have further questions or need more details about any specific model or their performance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the machine learning model they use in this study?\"\n",
    "answer = ask_question(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the deep learning model they use in this study?\n",
      "Answer: Based on the provided context, the study employs two types of deep learning models for the multiclass mental illness classification task:\n",
      "\n",
      "1. **Bidirectional Long Short-Term Memory (BiLSTM)**: This model is designed with two bidirectional recurrent layers with 256 and 128 units, respectively, followed by dropout layers for regularization. The model also includes a dense layer with 64 units and rectified linear unit activation, preceding the final output layer, which uses softmax activation for multiclass classification. The model is optimized using the Adam optimizer with a configurable learning rate.\n",
      "\n",
      "2. **Bidirectional Gated Recurrent Unit (BiGRU)**: This model shares the same architecture as the BiLSTM model, except for the type of recurrent layer used. It also comprises two bidirectional recurrent layers with 256 and 128 units, respectively, followed by dropout layers with a rate of 0.25 for regularization. Similarly, it includes a dense layer with 64 units and rectified linear unit activation, preceding the final output layer with softmax activation for multiclass classification. The model is also optimized using the Adam optimizer with a configurable learning rate.\n",
      "\n",
      "Both models utilize pretrained GloVe embeddings (glove.6B.300d.txt) with a 300-dimensional embedding layer and a maximum sequence length of 1024 tokens.\n",
      "\n",
      "The performance of these models was evaluated at different learning rates, with the BiLSTM and BiGRU models achieving their best performance at learning rates of 2e-3 and 1e-3, respectively.\n",
      "\n",
      "If you have further questions or need more details about any specific aspect of these models or their performance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the deep learning model they use in this study?\"\n",
    "answer = ask_question(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the paper title in this study?\n",
      "Answer: Based on the provided context and chat history, the paper title in this study is:\n",
      "\n",
      "**\"MIRoBERTa: Mental Illness Text Classification with Transfer Learning on Subreddits\"**\n",
      "\n",
      "The authors of this paper are:\n",
      "- MAVIN SAO\n",
      "- HOI-JEONG LIM\n",
      "\n",
      "Hoi-Jeong Lim is also identified as the corresponding author, with an email address of hjlim@jnu.ac.kr.\n",
      "\n",
      "The study focuses on the application of machine learning and deep learning models for mental illness text classification, utilizing data from Reddit subreddits. If you have any further questions about the study or need additional details, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the paper title in this study?\"\n",
    "answer = ask_question(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
